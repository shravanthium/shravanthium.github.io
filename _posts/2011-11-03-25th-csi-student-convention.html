---
layout: post
title: 25th CSI Student Convention
date: 2011-11-03 19:13:49.000000000 +05:30
categories:
- Bigdata
- Workshops and Conferences
tags: []
status: publish
type: post
published: true
meta:
  _edit_last: '21508824'
  _wpas_done_fb: '1'
  _wpas_skip_706781: '1'
  _wpas_skip_6773448: '1'
  _wpas_skip_2043461: '1'
  _wpas_skip_706780: '1'
author: 
---
<p>CSI, Computer Society of India conducted its 25th student convention at R.V College of Engineering on 13th and 14th of October,2011. I got an opportunity to be part of the convention and present our paper entitled " <strong>Map/Reduce Algorithm Performance Analysis in Computing Frequency of Tweets</strong> " along with my co-author Nagashree.</p>
<p>The convention was a great time for all the students who came from all across the state to learn about the latest trends in the field of Information Technology.Also it was a wonderful platform for innovative young minds to share there ideas and innovations.Students who came from different places of karnataka took part in the convention and presented there papers.</p>
<p>Hadoop and map/reduce being my area of interest we decided to present a paper on "<strong>Map/Reduce Algorithm Performance Analysis</strong>" so that more and more students get to know about this latest emerging technology.We were given just 10min to present our paper and we had only 10 min to impress the judges and to communicate our ideas with our counter friends who were present in the convention.It was a wonderful experience to present paper in front of eminent professionals who were the judges for the event between were quite nervous as it was our first ever paper presentation.The day became much more memorable when we got to know that we got 3rd place for our presentation.</p>
<p>Here is the abstract of our presentation:</p>
<h2 style="text-align:center;"><span style="text-decoration:underline;"> <strong><em> Abstract of Paper presentation </em></strong></span></h2>
<p><strong>Title:</strong><strong>Map/Reduce Algorithm Performance Analysis in Computing Frequency of Tweets</strong><strong><br />
</strong></p>
<p><strong>Background </strong></p>
<p>This paper proposes method to extract the tweets from twitter and analyses the efficiency of Map/Reduce algorithm on Hadoop framework hence achieves maximum performance.</p>
<p>New research in cloud computing has shown that implementing mapreduce not only influencing the performance -it also influences on more reliable storage management.</p>
<p>For about a decade it was considered that distributed computing is more complex to handle than expanding memory of single node cluster since inter-process communication (IPC) to be used to communicate with the nodes which was tedious to implement as the code would run longer than the computation procedure itself. But now apache.hadoop offers a more scalable and reliable platform to implement distributed computing .Through this paper we have analysed  that Map/Reduce algorithm run on hadoop  influences the performance significantly while handling huge data set stored on different nodes of a multi-node cluster .</p>
<p><strong>Aim of the study </strong></p>
<p>Cloud computing is the future and it will  focuses more on distributed computing. In order to evaluate the features offered by hadoop for cloud computing huge unstructured data set is required. The present study investigated those questions.</p>
<p>The main focus of the study was to analysis the performance of Map/Reduce algorithm in computing the frequency of tweets.</p>
<p><strong>Method </strong></p>
<p>About 6 to 10 lines of python algorithm was used to extract the tweets of people, taking input from twitter search API. Tweets were extracted consecutively for about 1 week resulted in a huge data set piling up to 50MB</p>
<p>The study was carried out in to parts. The first part was extracting tweets as mentioned above and the second was to implement customized Map/Reduce algorithm to compute the frequency of tweets on particular keywork(say “Anna Hazare”).</p>
<p><strong> Result</strong></p>
<p>It was found that this approach offers a more reliable method to analyse huge data compared to any other classic methods.</p>
<p>Here is the slides of our presentation:</p>
<p><a href="http://shravanthimohan.files.wordpress.com/2011/11/s2.png"><img class="aligncenter size-full wp-image-71" title="s2" src="assets/s2.png" alt="" width="600" height="452" /></a></p>
<p><a href="http://shravanthimohan.files.wordpress.com/2011/11/s3.png"><img class="aligncenter size-full wp-image-72" title="s3" src="assets/s3.png" alt="" width="600" height="451" /></a><a href="http://shravanthimohan.files.wordpress.com/2011/11/s4.png"><img class="aligncenter size-full wp-image-73" title="s4" src="assets/s4.png" alt="" width="600" height="452" /></a><a href="http://shravanthimohan.files.wordpress.com/2011/11/s5.png"><img class="aligncenter size-full wp-image-74" title="s5" src="assets/s5.png" alt="" width="600" height="451" /></a><a href="http://shravanthimohan.files.wordpress.com/2011/11/s6.png"><img class="aligncenter size-full wp-image-75" title="s6" src="assets/s6.png" alt="" width="600" height="452" /></a><a href="http://shravanthimohan.files.wordpress.com/2011/11/s7.png"><img class="aligncenter size-full wp-image-76" title="s7" src="assets/s7.png" alt="" width="600" height="453" /></a><a href="http://shravanthimohan.files.wordpress.com/2011/11/s9.png"><img class="aligncenter size-full wp-image-78" title="s9" src="assets/s9.png" alt="" width="600" height="450" /></a><a href="http://shravanthimohan.files.wordpress.com/2011/11/s10.png"><img class="aligncenter size-full wp-image-79" title="s10" src="assets/s10.png" alt="" width="600" height="449" /></a><a href="http://shravanthimohan.files.wordpress.com/2011/11/s11.png"><img class="aligncenter size-full wp-image-80" title="s11" src="assets/s11.png" alt="" width="600" height="452" /></a><a href="http://shravanthimohan.files.wordpress.com/2011/11/s12.png"><img class="aligncenter size-full wp-image-81" title="s12" src="assets/s12.png" alt="" width="600" height="450" /></a>Finally after the presentation I got to know that hadoop is the platform used for the India UID (ADAR Card) project and I felt proud for having the knowledge of it.</p>
